# -*- coding: utf-8 -*-
"""Steam Insight

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/157nB1rkD0tdSjYjksVX0pLlu4eRYRwjN

# Data Preprocessing

Import and Read Data From AirFlow as CSV
"""

import pandas as pd
import numpy as np

df = pd.read_csv('final_steam.csv')

df

"""# Data Modeling

## Collaborative Filtering User-Based
"""

!pip install scikit-surprise

from sklearn.metrics.pairwise import cosine_similarity
from surprise import Reader, Dataset, SVD
import matplotlib.pyplot as plt

# Ensure 'Genre' is split into a list for each game
df['Genre'] = df['Genre'].str.split(',')

# Create a one-hot encoded matrix for genres
genre_matrix = df.explode('Genre').pivot_table(
    index='AppID', columns='Genre', aggfunc='size', fill_value=0
)

# Compute genre similarity
genre_similarity = cosine_similarity(genre_matrix)
genre_similarity_df = pd.DataFrame(
    genre_similarity, index=genre_matrix.index, columns=genre_matrix.index
)

# Prepare collaborative filtering (CF) dataset
reader = Reader(rating_scale=(0, df['Log Playtime'].max()))
data_cf = Dataset.load_from_df(df[['SteamID64', 'AppID', 'Log Playtime']], reader)

# Train-test split
trainset_cf = data_cf.build_full_trainset()

# Train SVD model
svd_model = SVD(random_state=42)
svd_model.fit(trainset_cf)

# User-genre preferences
user_genre_preferences = (
    df.explode('Genre')
    .groupby(['SteamID64', 'Genre'])['Log Playtime']
    .sum()
    .unstack(fill_value=0)
)
user_genre_preferences = user_genre_preferences.div(user_genre_preferences.sum(axis=1), axis=0).fillna(0)

# Hybrid recommendation system
def hybrid_recommendations(user_id, top_n=5):
    # Get all unique games
    all_games = df['AppID'].unique()

    recommendations = []
    for game in all_games:
        # Predict playtime using CF model
        cf_prediction = svd_model.predict(user_id, game).est

        # Handle missing game genres
        if game not in genre_matrix.index:
            print(f"Warning: Game {game} not found in genre matrix. Skipping.")
            continue

        # Handle missing user preferences
        if user_id not in user_genre_preferences.index:
            print(f"Warning: User {user_id} not found in genre preferences. Skipping.")
            continue

        # Compute genre similarity weight
        game_genres = genre_matrix.loc[game]
        user_preferences = user_genre_preferences.loc[user_id]
        genre_weight = np.dot(game_genres.values, user_preferences.values)

        # Final hybrid score
        hybrid_score = cf_prediction + 0.3 * genre_weight  # Adjust genre weight as needed
        recommendations.append((game, hybrid_score))

    # Sort recommendations by score
    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)[:top_n]

    # Map AppIDs to Game Names
    appid_to_name = dict(zip(df['AppID'], df['Game Name']))
    recommendations = [(appid_to_name[game], score) for game, score in recommendations]
    return recommendations

# Visualize recommendations
def visualize_recommendations(recommendations, user_id):
    games, scores = zip(*recommendations)

    plt.figure(figsize=(10, 6))
    plt.barh(games, scores, color='skyblue')
    plt.gca().invert_yaxis()
    plt.xlabel('Hybrid Score')
    plt.ylabel('Game Name')
    plt.title(f"Top Recommendations for User {user_id}")
    plt.tight_layout()
    plt.show()

# Usage
user_id_input = input("Enter SteamID64: ")
try:
    user_id_input = int(user_id_input)
    hybrid_recs = hybrid_recommendations(user_id_input)

    print("\nTop Recommendations:")
    for game, score in hybrid_recs:
        print(f"Game: {game}, Hybrid Score: {score:.4f}")

    visualize_recommendations(hybrid_recs, user_id_input)
except ValueError:
    print("Invalid SteamID64. Please enter a numeric ID.")

"""## Collaborative Filtering Item-Based"""

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from collections import Counter

# explode it to create one row per genre per game
df_exploded = df.explode('Genre')

# Create a mapping from AppID to all genres (after explosion)
appid_to_genres = df_exploded.groupby('AppID')['Genre'].unique().to_dict()

# Create the interaction matrix (SteamID64 x AppID) using Log Playtime
interaction_matrix = df.pivot_table(index='SteamID64', columns='AppID', values='Log Playtime', aggfunc='sum', fill_value=0)

# Calculate the game count for each AppID
game_counts = df['AppID'].value_counts()

interaction_matrix.head()

# Subtract the user mean to normalize the playtime
user_means = interaction_matrix.mean(axis=1)
normalized_matrix = interaction_matrix.subtract(user_means, axis=0)

# Calculate cosine similarity between items (games) based on interactions
cosine_sim = cosine_similarity(interaction_matrix.T)

# Create a mapping from AppID to index
appid_to_index = {appid: idx for idx, appid in enumerate(interaction_matrix.columns)}

# Function to calculate similarity adjusted by genre and game count
def calculate_adjusted_similarity(game_idx, similarity_scores, appid, top_n=5):
    # Get the genre of the current game
    game_genres = appid_to_genres.get(appid, [])

    # Get the genres for all games
    all_game_genres = [appid_to_genres.get(interaction_matrix.columns[i], []) for i in range(len(similarity_scores))]

    # Calculate genre-based similarity weight
    genre_weights = [
        len(set(game_genres).intersection(set(all_game_genres[i]))) for i in range(len(similarity_scores))
    ]

    # Get the game count for the current game and all other games
    game_count = game_counts.get(appid, 0)
    game_count_weights = [game_counts.get(interaction_matrix.columns[i], 0) for i in range(len(similarity_scores))]

    # Adjust similarity scores by genre overlap and game count (this time without making scores equal)
    adjusted_scores = [
        similarity_scores[i] * (0.5 * genre_weights[i] + 0.5 * game_count_weights[i] / max(game_count_weights)) for i in range(len(similarity_scores))
    ]

    # Normalize scores to avoid extreme values (optional but can improve visualization)
    min_score = min(adjusted_scores)
    max_score = max(adjusted_scores)
    normalized_scores = [(score - min_score) / (max_score - min_score) for score in adjusted_scores]  # Normalize between 0 and 1

    return normalized_scores

# Function to recommend items (games) based on a given game (AppID)
def recommend_items(input_game_appid, top_n=5):
    if input_game_appid not in appid_to_index:
        print("Game not found in the dataset.")
        return []

    # Get the index of the input game
    game_idx = appid_to_index[input_game_appid]

    # Get the similarity scores of the input game with all other games
    similarity_scores = cosine_sim[game_idx]

    # Adjust similarity scores based on genre and game count
    adjusted_similarity_scores = calculate_adjusted_similarity(game_idx, similarity_scores, input_game_appid, top_n)

    # Get the indices of the top N most similar games
    similar_games_idx = np.argsort(adjusted_similarity_scores)[::-1][1:top_n+1]

    # Get the corresponding AppIDs for the similar games
    similar_games_appid = interaction_matrix.columns[similar_games_idx]

    # Return the recommendations as a list of AppIDs
    return similar_games_appid

# Function to get AppID from the game name
def get_appid_from_name(game_name):
    # Check if the game name exists in the dataframe
    appid = df[df['Game Name'] == game_name]['AppID'].unique()
    if len(appid) == 0:
        return f"Game '{game_name}' not found in the dataset."
    return appid[0]

# Main function to get user input and generate recommendations
def get_user_input_and_generate_recommendations():
    # Ask the user for the game name
    user_input_game = input("Enter the game name: ")

    # Get the AppID for the entered game name
    input_appid = get_appid_from_name(user_input_game)

    # Check if the game name is valid
    if isinstance(input_appid, str):  # If it's an error message
        print(input_appid)
    else:
        # Generate recommendations based on the AppID
        game_recommendations = recommend_items(input_appid)

        # Display recommendations in text format
        print(f"\nRecommendations for Game '{user_input_game}' (AppID: {input_appid}):")
        for game_appid in game_recommendations:
            game_name = df[df['AppID'] == game_appid]['Game Name'].iloc[0]
            print(f"Game: {game_name}, Similarity Score: {cosine_sim[appid_to_index[game_appid]].max():.4f}")

# Call the function to get user input and generate output
get_user_input_and_generate_recommendations()

"""## Clustering (Users)"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Ensure all values in 'Genre' are strings
df['Genre'] = df['Genre'].apply(lambda x: ', '.join(x) if isinstance(x, list) else str(x))

# Log-transform numerical columns with skewed distributions
df['Log Price'] = np.log1p(df['Price (IDR)'])  # Log-transform for price

# Select relevant columns
numerical_features = ['Log Playtime', 'Game Count', 'Current Players', 'Peak Players', 'Hours Played', 'Log Price']
categorical_features = ['Genre']

# Normalize numerical features
scaler = StandardScaler()
numerical_data = scaler.fit_transform(df[numerical_features])

# One-hot encode categorical features
encoder = OneHotEncoder(sparse_output=False)
categorical_data = encoder.fit_transform(df[categorical_features])

# Combine numerical and categorical features
processed_data = np.hstack([numerical_data, categorical_data])

# Perform PCA
pca = PCA()
pca_data = pca.fit_transform(processed_data)

# Explained Variance Ratio
explained_variance = pca.explained_variance_ratio_

# Cumulative Variance Plot
plt.figure(figsize=(10, 7))
plt.plot(np.cumsum(explained_variance), marker='o', linestyle='--')
plt.title('Cumulative Explained Variance by PCA Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.show()

# Determine number of components that explain at least 90% of the variance
threshold = 0.90
n_components_needed = np.argmax(np.cumsum(explained_variance) >= threshold) + 1

print(f"Number of PCA components needed to explain at least {threshold*100}% variance: {n_components_needed}")

# Apply PCA with the determined number of components
pca = PCA(n_components=n_components_needed)
pca_data = pca.fit_transform(processed_data)

# **Elbow Method** to determine the optimal number of clusters (k)
inertia = []
max_k = 10  # Trying k from 1 to 10

for k in range(1, max_k + 1):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(pca_data)
    inertia.append(kmeans.inertia_)

# Plot the Elbow Method
plt.figure(figsize=(8, 6))
plt.plot(range(1, max_k + 1), inertia, marker='o', color='b')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Sum of Squared Distances)')
plt.show()

# Based on the elbow plot, choose the optimal number of clusters
optimal_k = 2

# Clustering with K-Means
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(pca_data)

# Add cluster labels to the original data
df['User Cluster'] = clusters

# Visualize PCA Components
plt.figure(figsize=(10, 7))
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=clusters, cmap='viridis', s=50)
plt.colorbar(label='User Cluster')
plt.title('PCA Components and Clusters')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

# prompt: create pair plot for all the PCA Component

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'pca_data' is your PCA-transformed data and 'df' is your DataFrame
# Create a DataFrame from the PCA components
pca_df = pd.DataFrame(data=pca_data, columns=[f'PC{i+1}' for i in range(pca_data.shape[1])])

# Add the cluster labels to the PCA DataFrame
pca_df['User Cluster'] = df['User Cluster']

# Create the pairplot
sns.pairplot(pca_df, hue='User Cluster', diag_kind='kde')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.countplot(x='User Cluster', data=df)
plt.title('Cluster Distribution')
plt.xlabel('Cluster')
plt.ylabel('Count')
plt.show()

# Aggregating data by 'User Cluster' to find the top 5 games in each cluster
top_games_user_cluster = df.groupby(['User Cluster', 'Game Name']).size().reset_index(name='Counts')

# Get the top 5 games for each user cluster dynamically
top_games_user_cluster = top_games_user_cluster.groupby('User Cluster').apply(lambda x: x.nlargest(5, 'Counts')).reset_index(drop=True)

# Create subplots dynamically based on the number of clusters (rows = ceil(optimal_k/2), cols = 2)
import math
rows = math.ceil(optimal_k / 2)
fig, axes = plt.subplots(rows, 2, figsize=(12, rows * 4))

# Flatten axes for easier iteration (handles cases where optimal_k is not a multiple of 2)
axes = axes.flatten()

# Loop through each cluster and plot the top 5 games
for i in range(optimal_k):
    cluster_data = top_games_user_cluster[top_games_user_cluster['User Cluster'] == i]
    ax = axes[i]  # Select the correct subplot
    sns.barplot(x='Counts', y='Game Name', data=cluster_data, ax=ax, ci=None)
    ax.set_title(f'Top 5 Games for User Cluster {i}')
    ax.set_xlabel('Number of Users')
    ax.set_ylabel('Game Name')

# Remove any extra unused axes if k is less than the grid size
for j in range(optimal_k, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()